# ============================================================
# ğŸ“„ íŒŒì¼ëª…: ai_modules/src/train_efficientnet_baseline.py
# ğŸ“˜ ëª©ì :
#   - ì§ì ‘ ì •ì˜í•œ EfficientNetBaseline ëª¨ë¸ì„ ì´ìš©í•´
#     ì‚¬ì „ ì „ì²˜ë¦¬ëœ ì•½ ì´ë¯¸ì§€(JSON) ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ë° í‰ê°€ ìˆ˜í–‰.
#   - LightCNNë³´ë‹¤ í° ì…ë ¥ í¬ê¸°(128x128)ì™€ ë” ê¹Šì€ ë°±ë³¸ ì‚¬ìš©.
#   - í•™ìŠµ/ê²€ì¦ ë¶„í• , ëª¨ë¸ ì €ì¥, í…ŒìŠ¤íŠ¸ì…‹ 2ê°œ í‰ê°€, ì„±ëŠ¥ ê·¸ë˜í”„ í¬í•¨.
#
# ğŸ§ª ì‹¤í–‰ ì˜ˆì‹œ:
#   python -m ai_modules.src.train_efficientnet_baseline \
#     --train_json /content/gdrive/MyDrive/Matched/matched_train_90_original_noisy_sheared_bright.json \
#     --test_json1 /content/gdrive/MyDrive/Matched/fortest.json \
#     --test_json2 /content/gdrive/MyDrive/Matched/matched_test_18_deduped.json \
#     --save_dir /content/gdrive/MyDrive/ModelCheckpoints_baseline \
#     --epochs 30 --batch_size 32
# ============================================================

from __future__ import annotations
import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
import matplotlib.pyplot as plt

# âœ… ë„¤ê°€ ì§ì ‘ ì •ì˜í–ˆë˜ ëª¨ë“ˆë“¤
from ai_modules.src.data_prep.dataset_precomputed import PrecomputedPillDataset
from ai_modules.src.models.efficientnet_baseline import EfficientNetBaseline


# ------------------------------
# í•™ìŠµ ë£¨í”„
# ------------------------------
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_preds, total_labels = [], []
    loop = tqdm(loader, desc="ğŸŸ¢ Training", leave=False)

    for imgs, labels in loop:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model(imgs)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        preds = torch.argmax(outputs, dim=1)
        total_preds.extend(preds.cpu().numpy())
        total_labels.extend(labels.cpu().numpy())
        loop.set_postfix(loss=loss.item())

    acc = accuracy_score(total_labels, total_preds)
    f1 = f1_score(total_labels, total_preds, average='weighted')
    return acc, f1


# ------------------------------
# ê²€ì¦ ë£¨í”„
# ------------------------------
@torch.no_grad()
def evaluate(model, loader, device, idx2label=None):
    model.eval()
    total_preds, total_labels = [], []
    loop = tqdm(loader, desc="ğŸ”µ Evaluating", leave=False)

    for imgs, labels in loop:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model(imgs)
        preds = torch.argmax(outputs, dim=1)
        total_preds.extend(preds.cpu().numpy())
        total_labels.extend(labels.cpu().numpy())

    acc = accuracy_score(total_labels, total_preds)
    f1 = f1_score(total_labels, total_preds, average='weighted')

    if idx2label:
        idx_to_label = {int(k): v for k, v in idx2label.items()}
        target_names = [idx_to_label.get(i, str(i)) for i in range(len(idx_to_label))]
        print("\n[ğŸ” Classification Report]")
        print(classification_report(total_labels, total_preds, target_names=target_names, zero_division=0))

    return acc, f1


# ------------------------------
# í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
# ------------------------------
def plot_metrics(train_acc, val_acc, train_f1, val_f1):
    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1)
    plt.plot(train_acc, label='Train Acc')
    plt.plot(val_acc, label='Val Acc')
    plt.title("Accuracy")
    plt.legend(); plt.grid(True)

    plt.subplot(1,2,2)
    plt.plot(train_f1, label='Train F1')
    plt.plot(val_f1, label='Val F1')
    plt.title("F1 Score")
    plt.legend(); plt.grid(True)

    plt.tight_layout()
    plt.show()


# ------------------------------
# ë©”ì¸ ì‹¤í–‰ë¶€
# ------------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--train_json", required=True)
    ap.add_argument("--test_json1", default=None)
    ap.add_argument("--test_json2", default=None)
    ap.add_argument("--save_dir", default="./runs/efficientnet_baseline")
    ap.add_argument("--epochs", type=int, default=30)
    ap.add_argument("--batch_size", type=int, default=32)
    ap.add_argument("--lr", type=float, default=3e-4)
    args = ap.parse_args()

    os.makedirs(args.save_dir, exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ------------------------------
    # Transform ì •ì˜
    # ------------------------------
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
        transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),
    ])

    # ------------------------------
    # Dataset & Dataloader
    # ------------------------------
    train_val_dataset = PrecomputedPillDataset(args.train_json, transform=transform)
    n_train = int(0.8 * len(train_val_dataset))
    n_val = len(train_val_dataset) - n_train
    train_ds, val_ds = random_split(train_val_dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42))

    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True)
    val_loader   = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)

    test_loader_1, test_loader_2 = None, None
    if args.test_json1:
        test_loader_1 = DataLoader(PrecomputedPillDataset(args.test_json1, transform=transform), batch_size=args.batch_size)
    if args.test_json2:
        test_loader_2 = DataLoader(PrecomputedPillDataset(args.test_json2, transform=transform), batch_size=args.batch_size)

    num_classes = len(train_val_dataset.label2idx) if train_val_dataset.label2idx else (
        max(s['label'] for s in train_val_dataset.samples) + 1
    )

    # ------------------------------
    # ëª¨ë¸ ì´ˆê¸°í™”
    # ------------------------------
    model = EfficientNetBaseline(num_classes=num_classes, pretrained=True).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()

    best_val_acc, best_epoch = 0.0, -1
    train_acc_list, val_acc_list, train_f1_list, val_f1_list = [], [], [], []

    # ------------------------------
    # í•™ìŠµ ë£¨í”„
    # ------------------------------
    for epoch in range(args.epochs):
        print(f"\n[Epoch {epoch+1}/{args.epochs}]")
        train_acc, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_acc, val_f1 = evaluate(model, val_loader, device, idx2label=train_val_dataset.idx2label)

        train_acc_list.append(train_acc); val_acc_list.append(val_acc)
        train_f1_list.append(train_f1);   val_f1_list.append(val_f1)

        if val_acc > best_val_acc:
            best_val_acc, best_epoch = val_acc, epoch + 1
            torch.save(model.state_dict(), os.path.join(args.save_dir, "best_model.pth"))
            print("â­ï¸ Best model updated!")

        print(f"Train Acc: {train_acc:.2%} | F1: {train_f1:.4f}")
        print(f"Val   Acc: {val_acc:.2%} | F1: {val_f1:.4f}")

    print(f"\nğŸ í•™ìŠµ ì™„ë£Œ! Best Val Acc: {best_val_acc:.2%} (Epoch {best_epoch})")

    # ------------------------------
    # í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€
    # ------------------------------
    if test_loader_1:
        print("\nğŸ§ª [Test 1 - fortest.json]")
        test_acc1, test_f1_1 = evaluate(model, test_loader_1, device, idx2label=train_val_dataset.idx2label)
        print(f"Test 1 Acc: {test_acc1:.2%} | F1: {test_f1_1:.4f}")

    if test_loader_2:
        print("\nğŸ§ª [Test 2 - matched_test_18_deduped.json]")
        test_acc2, test_f1_2 = evaluate(model, test_loader_2, device, idx2label=train_val_dataset.idx2label)
        print(f"Test 2 Acc: {test_acc2:.2%} | F1: {test_f1_2:.4f}")

    # ------------------------------
    # í•™ìŠµ ê³¡ì„  ì¶œë ¥
    # ------------------------------
    plot_metrics(train_acc_list, val_acc_list, train_f1_list, val_f1_list)


if __name__ == "__main__":
    main()
